{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=float:right><img src=\"assets/img/appliedai-logo.png\" width=100 style=\"margin: 0px 20px\"></img></div>\n",
    "\n",
    "\n",
    "##### Jonathan Sedar Personal Project\n",
    "## PyMC3 vs PyStan Comparison\n",
    "_Spring 2016_\n",
    "\n",
    "This set of Notebooks and scripts comprise the **pymc3_vs_pystan** personal project by Jonathan Sedar of Applied AI Ltd, written primarily for presentation at the PyData London 2016 Conference.\n",
    "\n",
    "The project demonstrates hierarchical linear regression using two Bayesian inference frameworks: PyMC3 and PyStan. The project borrows heavily from code written for Applied AI Ltd and is supplied here for educational purposes only. No copyright or license is extended to users.\n",
    "\n",
    "\n",
    "    \n",
    "# 40_HierarchicalLinearRegression\n",
    "\n",
    "#### Demonstrate pooling and hierarchical linear regression\n",
    "\n",
    "\n",
    "Create a set of progressively more complex models, trying to show the effect of manufacturer upon NOx emissions. I'll evaluate the models using WAIC and PPC.\n",
    "\n",
    "+ [Setup](#Setup)\n",
    "    + [Local Functions](#Local-Functions)\n",
    "    + [Load Data](#Load-Data)\n",
    "    + [Prepare Dataset](#Prepare-Dataset)\n",
    "    + [Describe Dataset](#Describe-Dataset)\n",
    "\n",
    "\n",
    "+ [Choose Features](#Choose-Features)\n",
    "    + [Create Modelspecs and Design Matrices](#Create-Modelspecs-and-Design-Matrices)\n",
    "\n",
    "\n",
    "+ [Pooled Model](#Pooled-Model)\n",
    "\n",
    "\n",
    "+ [Unpooled Model](#Unpooled-Model)\n",
    "    + [Evaluate Manufacturers using Unpooled Model](#Evaluate-Manufacturers-using-Unpooled-Model)\n",
    "    \n",
    "    \n",
    "+ [Digression: Fully Unpooled Model](#Digression:-Fully-Unpooled-Model)\n",
    "\n",
    "\n",
    "+ [Partially-Pooled Model](#Partially-Pooled-Model)\n",
    "    + [Evaluate Manufacturers using Partially-Pooled Model](Evaluate-Manufacturers-using-Partially-Pooled-Model)\n",
    "    + [Can we comment on Volkswagen's NOx emissions at `mfr` level?](#Can-we-comment-on-Volkswagen's-NOx-emissions at `mfr` level?)\n",
    "\n",
    "\n",
    "+ [Hierarchical Model of Parent and Manufacturer](#Hierarchical-Model-of-Parent-and-Manufacturer)\n",
    "\n",
    "\n",
    "\n",
    "+ [Summary Insights and Notes](#Summary-Insights-and-Notes)\n",
    "    + [Model Comparison using WAIC](#Model-Comparison-using-WAIC)\n",
    "    + [Comparing Car Manufacturers and Parent Companies](Comparing-car-manufacturers-and-parent-companies)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+ [Digression: Model Comparison using WAIC](#Digression:-Model-Comparison-using-WAIC)\n",
    "\n",
    "\n",
    "+ [Final look at parent with a Partially-Pooled Model](#Final-look-at-parent-with-a-Partially-Pooled-Model)\n",
    "    + [Evaluate Manufacturers using Partially-Pooled `parent` Model](Evaluate-Manufacturers-using-Partially-Pooled-`parent`-Model)\n",
    "    + [Can we comment on Volkswagen's NOx emissions at `parent` level?](#Can-we-comment-on-Volkswagen's-NOx-emissions at `parent` level?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Interactive magics\n",
    "%matplotlib inline\n",
    "%qtconsole --colors=linux\n",
    "# %connect_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# general packages\n",
    "import sys\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "from convenience_functions import *\n",
    "from ipywidgets import interactive, fixed\n",
    "\n",
    "# scientific packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import patsy as pt\n",
    "from scipy import optimize\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.neighbors.kde import KernelDensity\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# pymc3 libraries\n",
    "import pymc3 as pm\n",
    "import theano as thno\n",
    "import theano.tensor as T \n",
    "import pystan\n",
    "\n",
    "# filter warnings for presentation's sake\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"darkgrid\", palette=\"muted\")\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "plt.rcParams['figure.figsize'] = 12, 4\n",
    "np.random.seed(0)\n",
    "\n",
    "dfwaic_pymc = pd.DataFrame() # setup for WAIC evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Package Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Python: {}'.format(sys.version))\n",
    "print('Recursion limit {}'.format(sys.getrecursionlimit()))\n",
    "print('theano: {}'.format(thno.__version__))\n",
    "print('PyMC3: {}'.format(pm.__version__))\n",
    "print('PyStan: {}'.format(pystan.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set switches for run-all convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_switches = {'pooled':{'pymc':1, 'stan':1},\n",
    "                   'unpooled':{'pymc':1, 'stan':1},\n",
    "                   'fullyunpooled':{'pymc':0},\n",
    "                   'partpooled':{'pymc':1, 'stan':1},\n",
    "                   'hier':{'pymc':1, 'stan':1}}\n",
    "\n",
    "runtimes = defaultdict(lambda : defaultdict(dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_smry(trc, dfs, pname='mfr'):\n",
    "    ''' Conv fn: create trace summary for sorted forestplot '''\n",
    "\n",
    "    dfsm = pm.df_summary(trc).reset_index()\n",
    "    dfsm.rename(columns={'index':'featval'}, inplace=True)\n",
    "    dfsm = dfsm.loc[dfsm['featval'].apply(\n",
    "        lambda x: re.search('{}__[0-9]+'.format(pname), x) is not None)]\n",
    "\n",
    "    dfsm.set_index(dfs[pname].unique(), inplace=True)\n",
    "    dfsm.sort('mean', ascending=True, inplace=True)\n",
    "    dfsm['ypos'] = np.arange(len(dfsm))\n",
    "    \n",
    "    return dfsm\n",
    "\n",
    "\n",
    "def custom_forestplot(df, sg, ylabel='mfr', size=8, aspect=0.8, facetby=None):\n",
    "    ''' Conv fn: plot features from pm.df_summary using seaborn\n",
    "        Facet on sets of forests for comparison '''\n",
    "\n",
    "    g = sns.FacetGrid(col=facetby, hue='mean', data=df, palette='RdBu_r'\n",
    "                      ,size=size, aspect=aspect, sharey=True)\n",
    "    _ = g.map(plt.scatter, 'mean', 'ypos'\n",
    "                ,marker='o', s=100, edgecolor='#333333', linewidth=0.8, zorder=10)\n",
    "    _ = g.map(plt.hlines, 'ypos', 'hpd_2.5','hpd_97.5', color='#aaaaaa')\n",
    "\n",
    "    _ = g.axes.flat[0].set_ylabel(ylabel)\n",
    "    _ = [ax.set_xlabel('coeff value') for ax in g.axes.flat]\n",
    "    _ = g.axes.flat[0].set_ylim((-1, df['ypos'].max()+1))\n",
    "    _ = g.axes.flat[0].set_yticks(np.arange(df['ypos'].max()+1))\n",
    "    _ = g.axes.flat[0].set_yticklabels(df.index)\n",
    "\n",
    "    if sg is not None:\n",
    "        # hacky way to show group sizes at rhs\n",
    "        sg = sg[df.iloc[:len(df)//len(g.axes.flat),:].index]    ## ensure index order\n",
    "        _ = axr = g.axes.flat[len(g.axes.flat)-1].twinx()\n",
    "        _ = axr.plot(np.zeros(len(sg)), range(len(sg)), lw=0)\n",
    "        _ = axr.set_ylim((-1,len(sg)))\n",
    "        _ = axr.set_yticks([l for l in range(len(sg))])\n",
    "        _ = axr.set_yticklabels(sg)\n",
    "        _ = axr.yaxis.grid(False)\n",
    "        _ = axr.spines['top'].set_visible(False)\n",
    "        _ = axr.spines['right'].set_visible(False)\n",
    "        \n",
    "    \n",
    "def custom_2d_forestplot(dfg, show='Zoom'):\n",
    "    ''' Conv fn: custom 2d forestplot of parent and mfr, optional CRs \n",
    "        NOTE legend hack to cope with open issue with FacetGrid hue and plt.scatter\n",
    "        https://github.com/mwaskom/seaborn/issues/840\n",
    "    \n",
    "    '''    \n",
    "\n",
    "    g = sns.FacetGrid(data=dfg, size=12, aspect=0.8)\n",
    "\n",
    "    _ = g.map(plt.scatter, 'mean_parent', 'mean_mfr',\n",
    "              marker='o', s=100, edgecolor='#333333', linewidth=0.8, zorder=10,\n",
    "              c=[sns.color_palette('Spectral', 20).as_hex()[m] for m in dfg['parent_order_id']])\n",
    "    \n",
    "    ylim_zoom = g.axes.flat[0].get_ylim()\n",
    "    xlim_zoom = g.axes.flat[0].get_xlim()\n",
    "    \n",
    "    _ = g.map(plt.hlines, 'mean_mfr', 'hpd_2.5_parent', 'hpd_97.5_parent', color='#bbbbbb')\n",
    "    _ = g.map(plt.vlines, 'mean_parent', 'hpd_2.5_mfr', 'hpd_97.5_mfr', color='#bbbbbb')\n",
    "\n",
    "    # hack autogen legend to show only parent dots\n",
    "    # hnd, lbl = g.axes.flat[0].get_legend_handles_labels()\n",
    "    # _ = g.axes.flat[0].legend(hnd[:20], lbl[:20], loc='upper left', ncol=1,\n",
    "    #     title='parent', frameon=True, framealpha=0.5).get_frame().set_facecolor('w')\n",
    "\n",
    "    # Manually create legend from custom artist/label lists\n",
    "    legcolors = [plt.Line2D([0,0],[0,1],\n",
    "                            color=sns.color_palette('Spectral', 20).as_hex()[p],\n",
    "                            ms=10, marker='o', mec='#333333', mew=1, linestyle='')\n",
    "                 for p in range(20)]\n",
    "    \n",
    "    _ = g.axes.flat[0].legend(legcolors,\n",
    "                              dfg['parent'].drop_duplicates().values.tolist(),\n",
    "                loc='upper left', ncol=1, title='parent', frameon=True, \n",
    "                framealpha=0.5, numpoints=1).get_frame().set_facecolor('w')\n",
    "\n",
    "    _ = g.axes.flat[0].set_ylabel('mfr')\n",
    "    _ = g.axes.flat[0].set_xlabel('parent')\n",
    "\n",
    "    for i, r in dfg[['mfr','n','mean_parent','mean_mfr']].iterrows():\n",
    "        _ = g.axes.flat[0].annotate('{} ({})'.format(r[0],r[1]), #.split(' - ')[1]\n",
    "                         xy=(r[2], r[3]), xycoords='data',\n",
    "                         xytext=(5,5), textcoords='offset points',\n",
    "                         color='#444444', fontsize=10, rotation=30, va='bottom')\n",
    "    if show == 'Zoom':\n",
    "        _ = g.axes.flat[0].set_ylim(ylim_zoom)\n",
    "        _ = g.axes.flat[0].set_xlim(xlim_zoom) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnxsql = sqlite3.connect('data/car_emissions.db')\n",
    "dfs = pd.read_sql('select * from cars_post_exclusions_2sd', cnxsql, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## convert sqlite bool storage (as ints) back to bools\n",
    "for ft in ['parent_is_vw', 'mfr_is_vw', 'is_tdi']:\n",
    "    dfs[ft] = dfs[ft].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_describe(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label encode `mfr` and `mfr_owner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "dfs['mfr_enc'] = le.fit_transform(dfs['mfr'])\n",
    "dfs['parent_enc'] = le.fit_transform(dfs['parent'])\n",
    "\n",
    "n_parent = dfs['parent_enc'].max()+1\n",
    "n_mfr = dfs['mfr_enc'].max()+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Declare feats for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fts_cat = ['parent_is_vw', 'mfr_is_vw', 'parent', 'mfr', 'trans', 'fuel_type', 'is_tdi']\n",
    "fts_cat_smp = ['mfr_is_vw','trans','fuel_type','is_tdi']\n",
    "fts_num = ['metric_combined', 'metric_extra_urban', 'metric_urban_cold'\n",
    "           ,'engine_capacity', 'emissions_co_mgkm']\n",
    "fts_num_smp = ['metric_combined', 'engine_capacity', 'emissions_co_mgkm']\n",
    "ft_endog = 'emissions_nox_mgkm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe dataset\n",
    "\n",
    "+ The dataset is 2593 rows, with 12 exog features, 1 endog feature.\n",
    "+ These are observations of car emissions tests, one row per car.\n",
    "+ You can read off the basic distributional statistics of the features in the table above. Numeric features have been standardized according to [Gelman's 2sd principle](http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf).\n",
    "+ I have selected these particular 12 features to work with. Some are derivatives of original features.\n",
    "\n",
    "We have the following features to choose from:\n",
    "\n",
    "```\n",
    "+ Categoricals:\n",
    "    + `trans`     - the car transmission, simplified to 'auto', 'semiauto', 'manual'\n",
    "    + `fuel_type` - the car power supply, simplified to 'petrol', 'diesel'\n",
    "    + `parent`    - the parent company of the car manufacturer, 20 values\n",
    "    + `mfr`       - the car manufacturer, 38 values\n",
    "\n",
    "+ Booleans:\n",
    "    + `parent_is_vw` - if the parent company of the car manufacturer is Volkswagen\n",
    "    + `mfr_is_vw`    - if the car manufacturer is Volkswagen\n",
    "    + `is_tdi`       - (processed feature) if the car engine type is a turbo diesel\n",
    "    \n",
    "+ Numerics:\n",
    "    + `metric_combined`    - a score for fuel efficiency in combined driving\n",
    "    + `metric_extra_urban` - a score for fuel efficiency in an extra-urban driving\n",
    "    + `metric_urban_cold`  - a score for fuel efficiency in an urban setting, cold start\n",
    "    + `emissions_co_mgkm`  - a count of CO particulates emitted mg/km\n",
    "    \n",
    "+ Numeric endogenous feature:\n",
    "    + `emissions_nox_mgkm` - a count of NOx particulates emitted mg/km    \n",
    "```\n",
    "\n",
    "For the purposes of this Notebook, the final feature mentioned `emissions_nox_mgkm` will be used as the _endogenous_ / _dependent_ / _output_ feature of the linear models. All other features may be used as _exogenous_ / _independent_ / _input_ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous Notebook, I used a Lasso model for feature reduction. I'll broadly follow the results of that exercise here and use the following features for modelling, I include `emissions_co_mgkm` just to demonstrate a continuous feature in there.\n",
    "\n",
    "Note: I will use this `glm` model specification for the pooled model. I will have to manually specify the unpooled, partially-pooled and hierarchical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "endogenous feature: emissions_nox_mgkm\n",
    "\n",
    "exogenous features: parent              : multi-class string\n",
    "                    mfr                 : multi-class string\n",
    "                    fuel_type           : multi-class string\n",
    "                    trans               : multi-class string\n",
    "                    is_tdi              : boolean\n",
    "                    engine_capacity     : numeric int\n",
    "                    metric_combined     : numeric int\n",
    "                    emissions_co_mgkm   : numeric float\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reminder of mfr and parent counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('parent: {} uniques\\nmfr: {} uniques'.format(\n",
    "        len(dfs['parent'].unique()), len(dfs['mfr'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Modelspecs and Design Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Only possible to use this formula for pooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fml_pooled = '{} ~ '.format(ft_endog) + ' + '.join(['fuel_type','trans'\n",
    "            ,'is_tdi','engine_capacity','metric_combined','emissions_co_mgkm'])\n",
    "print(fml_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(mx_en, mx_ex) = pt.dmatrices(fml_pooled, dfs\n",
    "                        ,return_type='dataframe', NA_action='raise')\n",
    "mx_ex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pool (ignore) the `parent` and `mfr` features.\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\beta^{T} \\bf{x},\\epsilon)$$\n",
    "\n",
    "where:  \n",
    "$\\beta$ are our coeffs in the linear model  \n",
    "$\\bf{x}$ is the vector of features describing each car in the dataset  \n",
    "$\\epsilon \\sim \\mathcal{HalfCauchy}(0, 10)$ \n",
    "\n",
    "I'll attempt to robustly handle outliers this time by using a Student-T distribution for the likelihood, the error-term $\\epsilon$ is stochastic noise in the likelihood of that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyMC3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create model and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['pooled']['pymc']:\n",
    "\n",
    "    with pm.Model() as mdl_pooled_pymc:\n",
    "\n",
    "        t0 = time()\n",
    "        pm.glm.glm(fml_pooled, dfs, family=pm.glm.families.StudentT())\n",
    "\n",
    "        runtimes['pooled']['pymc']['build'] = time() - t0\n",
    "        \n",
    "        start_map = pm.find_MAP(fmin=optimize.fmin_powell)\n",
    "        \n",
    "        runtimes['pooled']['pymc']['find_map'] = time() - runtimes['pooled']['pymc']['build']\n",
    "        \n",
    "        trc_pooled_pymc = pm.sample(2000, njobs=1, step=pm.NUTS(),\n",
    "                               start=start_map,\n",
    "                               trace=pm.backends.Text('traces/trc_pooled_pymc'))\n",
    "\n",
    "        runtimes['pooled']['pymc']['sample'] = time() - runtimes['pooled']['pymc']['find_map']\n",
    "        \n",
    "    ## dump to disk    \n",
    "    write_pickle(obj=mdl_pooled_pymc, relnm='models/mdl_pooled_pymc.pkl')\n",
    "    \n",
    "else:\n",
    "    with pm.Model():   ## read from disk\n",
    "\n",
    "        mdl_pooled_pymc = read_pickle(relnm='models/mdl_pooled_pymc.pkl')\n",
    "\n",
    "    trc_pooled_pymc = pm.backends.text.load('traces/trc_pooled_pymc', model=mdl_pooled_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save WAIC and view traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwaic_pymc['pooled'] = [pm.stats.waic(model=mdl_pooled_pymc, trace=trc_pooled_pymc[-1000:])]\n",
    "\n",
    "rvs_pooled = [rv.name for rv in strip_derived_rvs(mdl_pooled_pymc.unobserved_RVs)]\n",
    "plot_traces_pymc(trc_pooled_pymc[-1000:], varnames=rvs_pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe**:\n",
    "\n",
    "\n",
    "+ Stuff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyStan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_pooled_stan_spec = \"\"\"\n",
    "\n",
    "    /* Spec for pooled regression linear model (robust) */\n",
    "\n",
    "    data {                                 \n",
    "        int<lower=0> N;                     // count of observations\n",
    "        int<lower=0> K;                     // count of exog features\n",
    "        matrix[N, K] X;                     // exog features\n",
    "        vector[N] y;                        // endog feature\n",
    "    }\n",
    "    parameters {\n",
    "        vector[K] beta;                     // exog coeffs\n",
    "        real<lower=0> sigma;                // linear model error\n",
    "    }\n",
    "    transformed parameters {}\n",
    "    model {  \n",
    "        sigma ~ cauchy(0, 10);              // explicit prior for error (half-cauchy)\n",
    "        y ~ student_t(1, X * beta, sigma);  // student-T likelihood\n",
    "    }\n",
    "    generated quantities {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_pooled_stan_dict = {}\n",
    "mdl_pooled_stan_dict['N'] = mx_ex.shape[0]\n",
    "mdl_pooled_stan_dict['K'] = mx_ex.shape[1]\n",
    "mdl_pooled_stan_dict['X'] = mx_ex.values\n",
    "mdl_pooled_stan_dict['y'] = mx_en[ft_endog].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['pooled']['stan']:\n",
    "\n",
    "    mdl_pooled_stan_fit = pystan.stan(\n",
    "            model_code  = mdl_pooled_stan_spec,\n",
    "            data        = mdl_pooled_stan_dict,\n",
    "            sample_file = ensure_dir('traces/trc_pooled_stan/chain'),\n",
    "            model_name  = 'mdl_pooled_stan',\n",
    "            iter        = 1000,\n",
    "            warmup      = 750, \n",
    "            chains      = 4, \n",
    "            n_jobs      = 2, \n",
    "            verbose     = False)\n",
    "    \n",
    "    # dump model object and model fit object to disk\n",
    "    write_pickle(obj=mdl_pooled_stan_fit.get_stanmodel(),\n",
    "                 relnm='models/mdl_pooled_stan_model.pkl')\n",
    "\n",
    "    write_pickle(mdl_pooled_stan_fit, 'models/mdl_pooled_stan_fit.pkl')\n",
    "    \n",
    "else:      \n",
    "    ## load the model first, and then the fit:\n",
    "    mdl_pooled_stan_model = read_pickle(relnm='models/mdl_pooled_stan_model.pkl')\n",
    "\n",
    "    mdl_pooled_stan_fit = read_pickle(relnm='models/mdl_pooled_stan_fit.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mdl_pooled_stan_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl_pooled_stan_fit.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpooled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the `mfr` feature values in the dmatrix. Each `mfr` value gets a separate intercept with shared slopes.\n",
    "\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\beta_{mfr} + \\beta^{T} \\bf{x},\\epsilon)$$\n",
    "\n",
    "where:  \n",
    "$\\beta_{mfr}$ is a separate intercept for each manufacturer  \n",
    "$\\beta$ are our (shared) coeffs in the linear model  \n",
    "$\\bf{x}$ is the vector of features describing each car in the dataset  \n",
    "$\\epsilon \\sim \\mathcal{HalfCauchy}(0, 10)$ \n",
    "\n",
    "\n",
    "Set priors as Cauchy(0, 2.5) as per Gelman 2008? \n",
    "http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf\n",
    "\n",
    "Nope, in later correspondance, he recommends Normals http://andrewgelman.com/2015/11/01/cauchy-priors-for-logistic-regression-coefficients/\n",
    "\n",
    "Lots of other thoughts at: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyMC3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['unpooled']['pymc']:\n",
    "\n",
    "    with pm.Model() as mdl_unpooled_pymc:\n",
    "\n",
    "        b0 = pm.Normal('b0_mfr', mu=0, sd=10, shape=n_mfr)\n",
    "\n",
    "        b1 = pm.Normal('b1_fuel_type[T.petrol]', mu=0, sd=10)\n",
    "        b2a = pm.Normal('b2a_trans[T.manual]', mu=0, sd=10)\n",
    "        b2b = pm.Normal('b2b_trans[T.semiauto]', mu=0, sd=10)\n",
    "        b3 = pm.Normal('b3_is_tdi[T.True]', mu=0, sd=10)\n",
    "        b4 = pm.Normal('b4_engine_capacity', mu=0, sd=10)\n",
    "        b5 = pm.Normal('b5_metric_combined', mu=0, sd=10)\n",
    "        b6 = pm.Normal('b6_emissions_co_mgkm', mu=0, sd=10)\n",
    "\n",
    "        # define linear model\n",
    "        yest = ( b0[dfs['mfr_enc']] +\n",
    "                 b1 * mx_ex['fuel_type[T.petrol]'] + \n",
    "                 b2a * mx_ex['trans[T.manual]'] +\n",
    "                 b2b * mx_ex['trans[T.semiauto]'] +\n",
    "                 b3 * mx_ex['is_tdi[T.True]'] +\n",
    "                 b4 * mx_ex['engine_capacity'] +\n",
    "                 b5 * mx_ex['metric_combined'] +\n",
    "                 b6 * mx_ex['emissions_co_mgkm'])\n",
    "\n",
    "        ## Student T likelihood with HalfCauchy error and fixed DoF nu\n",
    "        epsilon = pm.HalfCauchy('epsilon', beta=10)\n",
    "        likelihood = pm.StudentT('likelihood', nu=1, mu=yest\n",
    "                                 ,sd=epsilon, observed=dfs[ft_endog])\n",
    " \n",
    "        ## sample\n",
    "        trc_unpooled_pymc = pm.sample(2000, njobs=2, step=pm.NUTS()\n",
    "                            ,start=pm.find_MAP(fmin=optimize.fmin_powell)     \n",
    "                            ,trace=pm.backends.Text('traces/trc_unpooled_pymc'))\n",
    "    ## dump to disk\n",
    "    write_pickle(obj=mdl_unpooled_pymc, relnm='models/mdl_unpooled_pymc.pkl')\n",
    "    \n",
    "else:  \n",
    "    with pm.Model():   ## read from disk\n",
    "\n",
    "        mdl_unpooled_pymc = read_pickle(relnm='models/mdl_unpooled_pymc.pkl')\n",
    "\n",
    "    trc_unpooled_pymc = pm.backends.text.load('traces/trc_unpooled_pymc',\n",
    "                                              model=mdl_unpooled_pymc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save WAIC and view traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwaic_pymc['unpooled'] = [pm.stats.waic(model=mdl_unpooled_pymc,\n",
    "                                         trace=trc_unpooled_pymc[-1000:])]\n",
    "\n",
    "rvs_unpooled_pymc = [rv.name for rv in strip_derived_rvs(mdl_unpooled_pymc.unobserved_RVs)]\n",
    "plot_traces_pymc(trc_unpooled_pymc[-1000:], varnames=rvs_unpooled_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyStan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Create model spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_unpooled_stan_spec = \"\"\"\n",
    "\n",
    "    /* Spec for unpooled regression linear model */\n",
    "\n",
    "    data {                                 \n",
    "        int<lower=0> N;                           // count of observations\n",
    "        int<lower=0> K;                           // count of exog features\n",
    "        matrix[N, K] X;                           // exog features\n",
    "        vector[N] y;                              // endog feature\n",
    "        int<lower=0> n_mfr;                   // count of mfr index levels\n",
    "        int<lower=1,upper=n_mfr> mfr_enc[N];  // mfr index encoding  \n",
    "    }\n",
    "    parameters {\n",
    "        vector[n_mfr] b0_mfr;                 // mfr intercept coeff\n",
    "        vector[K] beta;                           // exog coeffs\n",
    "        real<lower=0> sigma;                      // linear model error\n",
    "    }\n",
    "    transformed parameters {}\n",
    "    model {  \n",
    "        sigma ~ cauchy(0, 10);                    // prior for error\n",
    "        y ~ student_t(1, b0_mfr[mfr_enc] + X * beta, sigma);  // student-T likelihood\n",
    "    }\n",
    "    generated quantities {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl_unpooled_stan_dict = {}\n",
    "mdl_unpooled_stan_dict['N'] = mx_ex.iloc[:,1:].shape[0]\n",
    "mdl_unpooled_stan_dict['K'] = mx_ex.iloc[:,1:].shape[1]\n",
    "mdl_unpooled_stan_dict['X'] = mx_ex.iloc[:,1:].values\n",
    "mdl_unpooled_stan_dict['y'] = mx_en[ft_endog].values\n",
    "\n",
    "mdl_unpooled_stan_dict['mfr_enc'] = dfs['mfr_enc'].values + 1\n",
    "mdl_unpooled_stan_dict['n_mfr'] = n_mfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['unpooled']['stan']:\n",
    "\n",
    "    mdl_unpooled_stan_fit = pystan.stan(\n",
    "            model_code  = mdl_unpooled_stan_spec,\n",
    "            data        = mdl_unpooled_stan_dict,\n",
    "            sample_file = ensure_dir('traces/trc_unpooled_stan/chain'),\n",
    "            model_name  = 'mdl_unpooled_stan',\n",
    "            iter        = 1000,\n",
    "            warmup      = 750, \n",
    "            chains      = 4, \n",
    "            n_jobs      = 2, \n",
    "            verbose     = False)\n",
    "    \n",
    "    # dump model object and model fit object to disk\n",
    "    write_pickle(obj=mdl_unpooled_stan_fit.get_stanmodel(),\n",
    "                 relnm='models/mdl_unpooled_stan_model.pkl')\n",
    "\n",
    "    write_pickle(mdl_unpooled_stan_fit, 'models/mdl_unpooled_stan_fit.pkl')\n",
    "    \n",
    "else:      \n",
    "    ## load the model first, and then the fit:\n",
    "    mdl_unpooled_stan_model = read_pickle(relnm='models/mdl_unpooled_stan_model.pkl')\n",
    "\n",
    "    mdl_unpooled_stan_fit = read_pickle(relnm='models/mdl_unpooled_stan_fit.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mdl_unpooled_stan_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdl_unpooled_stan_fit.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Manufacturers using Unpooled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View forestplot of the `mfr` feature coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsm_unpl_mfr = create_smry(trc_unpooled_pymc[-1000:], dfs, 'mfr')\n",
    "sg_mfrfreq = dfs.groupby('mfr').size().reindex(dfsm_unpl_mfr.index)\n",
    "\n",
    "custom_forestplot(dfsm_unpl_mfr, sg_mfrfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "The forestplot lets us compare the effect of `mfr` upon `emissions_nox_mgkm` when all other features in the mode are kept equal: `engine_capacity`, `fuel_type` etc etc\n",
    "\n",
    "The forest plot shows:\n",
    "\n",
    "+ The mean value for each parameter value, sorted in descending order\n",
    "+ The uncertainty in each value, a.k.a. the Credible Region (CR) the region which the sample values spend X% of their time durng the traces. By convention I have chosen the 95% CR, but we could chose the 50% region or indeed anything that is useful for understanding.\n",
    "\n",
    "Looking at the manufacturer values:\n",
    "\n",
    "+ Mitsubuishi, seems to hang outside the pack by quite a long way\n",
    "+ Jaguar and Lexus appear to emit least, however, you can see a weakness in this unpooled model, which is the massive uncertainty region for under-represented manufacturers\n",
    "\n",
    "Under-representation is a problem:\n",
    "\n",
    "+ Lamborghini and Lexus are represented in the dataset by 1 and 5 cars respectively and have massive CRs, so wide that we really can't say much about their effect upon emissions with certainty\n",
    "+ Other manufacturers with few cars also have wide CRs: Mistubuishi (4), Ssangyong (5), Ferrari (2), Aston Martin Lagonda (6), and Smart (5)\n",
    "\n",
    "We'll see later how this can be improved using partially-pooled model with a shared hyperparameter to 'share power' between the manufacturer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digression: Fully Unpooled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can take this unpooling to an extreme, calculating separate intercepts and slopes for each manufacturer:\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\beta_{mfr}^{T} \\bf{x},\\epsilon)$$\n",
    "\n",
    "where:  \n",
    "$\\beta_{mfr}$ are separate coeffs for each manufucturer\n",
    "$\\bf{x}$ is the vector of features describing each car in the dataset  \n",
    "$\\epsilon \\sim \\mathcal{HalfCauchy}(0, 10)$ \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyMC3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['fullyunpooled']['pymc']:\n",
    "\n",
    "    with pm.Model() as mdl_fullyunpooled_pymc:\n",
    "\n",
    "        b0 = pm.Normal('b0_mfr', mu=0, sd=10, shape=n_mfr)\n",
    "        b1 = pm.Normal('b1_fuel_type[T.petrol]', mu=0, sd=10, shape=n_mfr)\n",
    "        b2a = pm.Normal('b2a_trans[T.manual]', mu=0, sd=10, shape=n_mfr)\n",
    "        b2b = pm.Normal('b2b_trans[T.semiauto]', mu=0, sd=10, shape=n_mfr)\n",
    "        b3 = pm.Normal('b3_is_tdi[T.True]', mu=0, sd=10, shape=n_mfr)\n",
    "        b4 = pm.Normal('b4_engine_capacity', mu=0, sd=10, shape=n_mfr)\n",
    "        b5 = pm.Normal('b5_metric_combined', mu=0, sd=10, shape=n_mfr)\n",
    "        b6 = pm.Normal('b6_emissions_co_mgkm', mu=0, sd=10, shape=n_mfr)\n",
    "\n",
    "        # define linear model\n",
    "        yest = ( b0[dfs['mfr_enc']] +\n",
    "                 b1[dfs['mfr_enc']] * mx_ex['fuel_type[T.petrol]'] + \n",
    "                 b2a[dfs['mfr_enc']] * mx_ex['trans[T.manual]'] +\n",
    "                 b2b[dfs['mfr_enc']] * mx_ex['trans[T.semiauto]'] +\n",
    "                 b3[dfs['mfr_enc']] * mx_ex['is_tdi[T.True]'] +\n",
    "                 b4[dfs['mfr_enc']] * mx_ex['engine_capacity'] +\n",
    "                 b5[dfs['mfr_enc']] * mx_ex['metric_combined'] +\n",
    "                 b6[dfs['mfr_enc']] * mx_ex['emissions_co_mgkm'])\n",
    "\n",
    "        ## Student T likelihood with fixed degrees of freedom nu\n",
    "        epsilon = pm.HalfCauchy('epsilon', beta=10)\n",
    "        likelihood = pm.StudentT('likelihood', nu=1, mu=yest\n",
    "                                 ,sd=epsilon, observed=dfs[ft_endog])\n",
    "\n",
    "        ## sample\n",
    "        trc_fullyunpooled_pymc = pm.sample(10000, njobs=3, step=pm.Metropolis()\n",
    "                                ,start=pm.find_MAP(fmin=optimize.fmin_powell)    \n",
    "                                ,trace=pm.backends.Text('traces/trc_fullyunpooled_pymc'))\n",
    "\n",
    "    ## dump to disk\n",
    "    write_pickle(obj=mdl_fullyunpooled_pymc, relnm='models/mdl_fullyunpooled_pymc.pkl')\n",
    "    \n",
    "else:\n",
    "    with pm.Model():    ## read from disk\n",
    "\n",
    "        mdl_fullyunpooled_pymc = read_pickle(relnm='models/mdl_fullyunpooled_pymc.pkl')\n",
    "\n",
    "    trc_fullyunpooled_pymc = pm.backends.text.load('traces/trc_fullyunpooled_pymc',\n",
    "                                                   model=mdl_fullyunpooled_pymc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwaic_pymc['fullyunpooled'] = [pm.stats.waic(model=mdl_fullyunpooled_pymc,\n",
    "                                         trace=trc_fullyunpooled_pymc[-333:])]\n",
    "rvs_fullyunpooled_pymc = [rv.name for rv in \\\n",
    "                          strip_derived_rvs(mdl_fullyunpooled_pymc.unobserved_RVs)]\n",
    "plot_traces_pymc(trc_fullyunpooled_pymc[-333:], varnames=rvs_fullyunpooled_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "There's three big issues with this fully unpooled model:\n",
    "\n",
    "1. You'll notice I used the Metropolis sampler, rather than NUTS, because the NUTS sampler seemed to 'stall' and fail to move quickly around the posterior distribution - it sampled so slowly that it's unsuitable for this short demo.\n",
    "2. Relatedly, the traces often show extreme values for parameters: likely because the separate values per manufacturer are simply allowed to vary too much and cause discontinuities in the posterior distribution.\n",
    "3. Now, the differences between the manufacturers are captured across _all_ parameters in the mode, which makes comparing them really difficult! The mode may fit better (see the WAIC evaluation below), but we've made the task of human interpretation more difficult.\n",
    "\n",
    "We have effectively fitted 38 seprate regressions, leading to immense complexity, slow sampling and messy traces. For this model at least, we need some degree of pooling.\n",
    "\n",
    "\n",
    "**NOTE**: Regarding point 2 above: the slowness of NUTS sampling may possibly be something to do with the implementation in PyMC3, and I will look into this in future comparisons with Stan (via PyStan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially-Pooled Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we place partial-pooling on intercept only: this hyperparameter lets us - in a balanced way - determine a difference between manufacturers `mfr` $m \\in manufacturer$, keeping all other features constant\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\beta_{mfr} + \\beta^{T} \\bf{x}, \\epsilon)$$\n",
    "\n",
    "where (tree written upside down):  \n",
    "$\\beta_{mfr} \\sim \\mathcal{N}(\\mu_{mfr}, \\sigma_{mfr})$\n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;|\\_\\_ \\mu_{mfr} \\sim \\mathcal{N}(0, 100) \\;\\;;\\;\\;\n",
    "\\sigma_{mfr} \\sim \\mathcal{HalfCauchy}(0, 10)$ \n",
    "\n",
    "$\\beta$ are the other (shared) coeffs in the linear model  \n",
    "$\\bf{x}$ is the vector of features describing each car in the dataset  \n",
    "$\\epsilon \\sim \\mathcal{HalfCauchy}(0, 10)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyMC3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['partpooled']['pymc']:\n",
    "\n",
    "    with pm.Model() as mdl_partpooled_pymc:\n",
    "\n",
    "        # define hyperpriors for intercept\n",
    "        b0_mu = pm.Normal('b0_mu', mu=0, sd=10)\n",
    "        b0_sd = pm.HalfCauchy('b0_sd', beta=10)\n",
    "\n",
    "        # define priors\n",
    "        b0 = pm.Normal('b0_mfr', mu=b0_mu, sd=b0_sd, shape=n_mfr)\n",
    "        b1 = pm.Normal('b1_fuel_type[T.petrol]', mu=0, sd=10)\n",
    "        b2a = pm.Normal('b2a_trans[T.manual]', mu=0, sd=10)\n",
    "        b2b = pm.Normal('b2b_trans[T.semiauto]', mu=0, sd=10)\n",
    "        b3 = pm.Normal('b3_is_tdi[T.True]', mu=0, sd=10)\n",
    "        b4 = pm.Normal('b4_engine_capacity', mu=0, sd=10)\n",
    "        b5 = pm.Normal('b5_metric_combined', mu=0, sd=10)\n",
    "        b6 = pm.Normal('b6_emissions_co_mgkm', mu=0, sd=10)\n",
    "\n",
    "        # define linear model\n",
    "        yest = ( b0[dfs['mfr_enc']] +\n",
    "                 b1 * mx_ex['fuel_type[T.petrol]'] + \n",
    "                 b2a * mx_ex['trans[T.manual]'] +\n",
    "                 b2b * mx_ex['trans[T.semiauto]'] +\n",
    "                 b3 * mx_ex['is_tdi[T.True]'] +\n",
    "                 b4 * mx_ex['engine_capacity'] +\n",
    "                 b5 * mx_ex['metric_combined'] +\n",
    "                 b6 * mx_ex['emissions_co_mgkm'])\n",
    "\n",
    "        ## Student T likelihood with fixed degrees of freedom nu\n",
    "        epsilon = pm.HalfCauchy('epsilon', beta=10)\n",
    "        likelihood = pm.StudentT('likelihood', nu=1, mu=yest\n",
    "                                 ,sd=epsilon, observed=dfs[ft_endog])\n",
    "\n",
    "        ## sample\n",
    "        trc_partpooled_pymc = pm.sample(1000, njobs=2, step=pm.NUTS(),\n",
    "                                start=pm.find_MAP(fmin=optimize.fmin_powell),\n",
    "                                trace=pm.backends.Text('traces/trc_partpooled_pymc'))\n",
    "    ## dump to disk\n",
    "    write_pickle(obj=mdl_partpooled_pymc, relnm='models/mdl_partpooled_pymc.pkl')\n",
    "    \n",
    "else:\n",
    "    with pm.Model():   ## read from disk\n",
    "\n",
    "        mdl_partpooled_pymc = read_pickle(relnm='models/mdl_partpooled_pymc.pkl')\n",
    "\n",
    "    trc_partpooled_pymc = pm.backends.text.load('traces/trc_partpooled_pymc',\n",
    "                                                   model=mdl_partpooled_pymc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save WAIC and View traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwaic_pymc['partpooled'] = [pm.stats.waic(model=mdl_partpooled_pymc,\n",
    "                                         trace=trc_partpooled_pymc[-500:])]\n",
    "rvs_partpooled_pymc = [rv.name for rv in \\\n",
    "                          strip_derived_rvs(mdl_partpooled_pymc.unobserved_RVs)]\n",
    "plot_traces_pymc(trc_partpooled_pymc[-500:], varnames=rvs_partpooled_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Observe:**\n",
    "\n",
    "This is more like it:\n",
    "\n",
    "+ The traceplots look pretty well-mixed\n",
    "+ We have a shared mean for the intercept `b0_mu` at approx. 48.\n",
    "+ All the 38 `mfr` values are located around this value, with standard deviation `b0_sd`: as we see in the plot for `b0_mfr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyStan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_partpooled_stan_spec = \"\"\"\n",
    "\n",
    "    /* Spec for partpooled regression linear model */\n",
    "\n",
    "    data {                                 \n",
    "        int<lower=0> N;                         // count of observations\n",
    "        int<lower=0> K;                         // count of exog features\n",
    "        matrix[N, K] X;                         // exog features\n",
    "        vector[N] y;                            // endog feature\n",
    "        int<lower=0> n_mfr;                     // count of mfr index levels\n",
    "        int<lower=1, upper=n_mfr> mfr_enc[N];   // mfr index encoding  \n",
    "    }\n",
    "    parameters {\n",
    "        vector[K] beta;                         // exog coeffs\n",
    "        real<lower=0> sigma;                    // linear model error       \n",
    "\n",
    "        real mfr_mu;                            // part-pooling prior mu\n",
    "        real<lower=0> mfr_sd;                   // part-pooling prior sd    \n",
    "        vector[n_mfr] b0_mfr;                   // mfr intercept coeff\n",
    "    }\n",
    "    transformed parameters {}\n",
    "    model {  \n",
    "        \n",
    "        mfr_mu ~ normal(0, 10);                  // part-pooling prior mu\n",
    "        mfr_sd ~ cauchy(0, 10);                  // part-pooling prior sd\n",
    "     \n",
    "        for (mfr in 1:n_mfr) {\n",
    "              b0_mfr[mfr] ~ normal(mfr_mu, mfr_sd);\n",
    "        }\n",
    "    \n",
    "        sigma ~ cauchy(0, 10);                    // prior for error\n",
    "        y ~ student_t(1, b0_mfr[mfr_enc] + X * beta, sigma);  // student-T likelihood\n",
    "    }\n",
    "    generated quantities {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['partpooled']['stan']:\n",
    "\n",
    "    mdl_partpooled_stan_fit = pystan.stan(\n",
    "            model_code  = mdl_partpooled_stan_spec,\n",
    "            data        = mdl_unpooled_stan_dict,\n",
    "            sample_file = ensure_dir('traces/trc_partpooled_stan/chain'),\n",
    "            model_name  = 'mdl_partpooled_stan',\n",
    "            iter        = 1000,\n",
    "            warmup      = 750, \n",
    "            chains      = 4, \n",
    "            n_jobs      = 2, \n",
    "            verbose     = False)\n",
    "    \n",
    "    # dump model object and model fit object to disk\n",
    "    write_pickle(obj=mdl_partpooled_stan_fit.get_stanmodel(),\n",
    "                 relnm='models/mdl_partpooled_stan_model.pkl')\n",
    "\n",
    "    write_pickle(mdl_partpooled_stan_fit, 'models/mdl_partpooled_stan_fit.pkl')\n",
    "    \n",
    "else:      \n",
    "    ## load the model first, and then the fit:\n",
    "    mdl_partpooled_stan_model = read_pickle(relnm='models/mdl_partpooled_stan_model.pkl')\n",
    "\n",
    "    mdl_partpooled_stan_fit = read_pickle(relnm='models/mdl_partpooled_stan_fit.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mdl_partpooled_stan_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = mdl_partpooled_stan_fit.plot()\n",
    "f.set_size_inches(12, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare PyStan model coeffs to PyMC3 coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return a dictionary of arrays\n",
    "samples = mdl_partpooled_stan_fit.extract(permuted=True)\n",
    "for k, v in samples.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(samples['beta'], columns=mx_ex.columns[1:]).describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pm.df_summary(trc_partpooled_pymc[-500:], varnames=rvs_partpooled_pymc).iloc[-9:-2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Looks similar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Manufacturers using Partially-Pooled PyMC3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the forestplot for this partpooled model with that of the unpooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsm_ptpl_mfr = create_smry(trc_partpooled_pymc[-500:], dfs, 'mfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsm_mfr_vs = pd.concat((dfsm_ptpl_mfr\n",
    "                         ,dfsm_unpl_mfr.reindex(dfsm_ptpl_mfr.index)), axis=0)\n",
    "dfsm_mfr_vs['mdl'] = np.concatenate(\n",
    "                    (np.repeat(['partpooled'],38), np.repeat('unpooled',38)))\n",
    "dfsm_mfr_vs.iloc[38:]['ypos'] = dfsm_mfr_vs.iloc[:38]['ypos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_forestplot(dfsm_mfr_vs, sg_mfrfreq, aspect=0.6, facetby='mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ The forestplot for the `partpooled` model is shown on the left and the `unpooled` is on the right. Note the rows are ordered according to the `partpooled` model.\n",
    "+ There's a few small changes in ordering from the `unpooled` model, for instance: Ssangyong, Mercedes-Benz, Ferrari\n",
    "+ There's a noticable reduction in uncertainty for some parameters which have low counts, for instance: Lamborghini (1 car), Lexus (5 cars) and Smart (5 cars)\n",
    "\n",
    "**Shrinkage**\n",
    "\n",
    "Overall all the parameters appear to be pulled in slightly closer together, this is a.k.a 'shrinkage'\n",
    "+ The coeffs now occupy a region between 34 - 62, centered on approx 50. This compares to the unpooled model where parameters have a region between 32 and 72. \n",
    "+ This reduction in variance would suggest the `partpooled` model is less overfitted than the `unpooled`, and may perform better in hold-out validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Can we comment on Volkswagen's NOx emissions at `mfr` level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that the intercept parameter for `mfr == volkswagen` is 8th highest in the pack of all 38 manufacturers, seemingly higher than average.\n",
    "\n",
    "Let's take a more detailled look at the parameter value compared to the group mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Summary of Volkswagen\n",
    "\n",
    "dfsm_ptpl_mfr.loc[['volkswagen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Hyperprior group mean and standard dev\n",
    "\n",
    "pm.df_summary(trc_partpooled_pymc[-500:], varnames=['b0_mu','b0_sd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "The model is specified such that all 38 `mfr` parameters share a common hyperparameter `b0_mu` for their mean, and common hyperparameter `b0_sd` for their standard deviation.\n",
    "\n",
    "+ Looking at the **mean**:  \n",
    "Volkswagen has a mean value of `55.32` with a 95% CR from `52.12 to 58.15`  \n",
    "The group mean `b0_mn` has a mean value much lower at `48.15` with a 95% CR from `44.76 to 51.80`  \n",
    "The 95% CR for Volkswagen does not overlap with the 95% CR for the group mean, so we can say that it is strongly above the mean for NOx emissions.\n",
    "\n",
    "\n",
    "+ Looking at the **standard deviation**:  \n",
    "Volkswagen has a sd value of `1.60`   \n",
    "The group sd `b0_sd` has a mean value much higher at `8.71` with a 95% CR from `6.34 to 10.99`  \n",
    "The sd for Volkswagen does not overlap with the 95% CR for the group sd, so we can say that Volkswagen has a strongly narrow distribution in its mean NOx emissions.\n",
    "\n",
    "\n",
    "+ **In summary**  it seems that Volkswagen has an unusually high and tight parameter explaining their NOx emissions.\n",
    "\n",
    "\n",
    "\n",
    "**Caveat: This is far from rigorous!**  \n",
    "\n",
    "1. The manufacturer parameter values are all quite well distributed through a large range `34 to 62` and don't always overlap with one another. We can make the same inference of 'tightly above average NOx emissions' for several other manufacturers: Rolls Royce, Fiat, Alfa Romeo, Maclaren and Subaru.\n",
    "2. We haven't considered the manufacturer-owner parameter `mfr_owner`: which is higher-level information and may help with the class imbalances on `mfr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Model of Parent and Manufacturer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I wanted to build the case slowly, I've not yet considered `parent`: the manufacturer parent company.\n",
    "\n",
    "Now's the time to consider it: because the parent company may provide important information. \n",
    "\n",
    "We could create another part-pooled model like above, with `parent` as a separate set of parameter coeffs, but quite obviously the `mfr` correolates completely with `parent` in a hierarchical structure; we can't assume orthogonality in the model.\n",
    "\n",
    "Rather than have all `mfr`s joined by a single hyperparameter, let's instead join them to a hyperparameter set by their parent company `parent`. These 20 `parent` levels will in turn be joined to a single hyperparameter. It's just an extension of the part-pooling model to create a truely hierarchical model which incorporates the ownership structure we know about. \n",
    "\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\beta_{mfr} + \\beta^{T} \\bf{x}, \\epsilon)$$\n",
    "\n",
    "where (tree written upside down):  \n",
    "$\\beta_{mfr} \\sim \\mathcal{N}(\\mu_{mfr}, \\sigma_{mfr})$\n",
    "\n",
    "$\\;\\;\\;\\;|\\_\\_ \\mu_{mfr} \\sim \\mathcal{N}(\\mu_{parent}, \\sigma_{parent}) \\;\\; ; \\;\\;\n",
    "\\sigma_{mfr} \\sim \\mathcal{HalfCauchy}(0, 10)$ \n",
    "\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;|\\_\\_  \\mu_{parent} \\sim \\mathcal{N}(0, 10) \\;\\; ; \\;\\; \\sigma_{parent}\\sim \\mathcal{HalfCauchy}(0, 10)$  \n",
    "\n",
    "\n",
    "$\\beta$ are the other (shared) coeffs in the linear model  \n",
    "$\\bf{x}$ is the vector of features describing each car in the dataset  \n",
    "$\\epsilon \\sim \\mathcal{HalfCauchy}(0, 10)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfr_parent_map = (dfs.groupby(['mfr_enc','parent_enc']).size()\n",
    "                  .reset_index()['parent_enc'].values)\n",
    "mfr_parent_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyMC3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['hier']['pymc']:\n",
    "\n",
    "#     @thno.compile.ops.as_op(itypes=[thno.tensor.dvector, thno.tensor.iscalar],\n",
    "#                             otypes=[thno.tensor.dvector])\n",
    "#     def mfr2parent(b0_parent=b0_parent, phi=phi):\n",
    "#         return [phi[z[i,j]] for i in range(D) for j in range(W)]\n",
    "    \n",
    "    time_mdl_hier_pymc = {}\n",
    "    t0 = time()\n",
    "    \n",
    "    with pm.Model() as mdl_hier_pymc:\n",
    "\n",
    "        # set theano constants for convenient theano object indexing\n",
    "        #parent_enc = T.constant(dfs['parent_enc'])\n",
    "\n",
    "        # define hyperpriors for intercept based on parent   # 1x \n",
    "        b0_parent_mn = pm.Normal('b0_parent_mn', mu=0, sd=10)\n",
    "        b0_parent_sd = pm.HalfCauchy('b0_parent_sd', beta=10)\n",
    "\n",
    "        # define hyperpriors for mfr based on parent         # 20x\n",
    "        b0_parent = pm.Normal('b0_parent', mu=b0_parent_mn,\n",
    "                              sd=b0_parent_sd, shape=n_parent)\n",
    "        b0_mfr_sd = pm.HalfCauchy('b0_mfr_sd', beta=10)\n",
    "\n",
    "        # define priors\n",
    "\n",
    "        # b0 vectorised version, this is definitely wrong get broadcast issues,\n",
    "        # I've no idea how to force it to index mfr -> parent correctly\n",
    "        b0 = pm.Normal('b0_mfr',\n",
    "                        mu=b0_parent[mfr_parent_map],\n",
    "                        sd=b0_mfr_sd, shape=n_mfr)\n",
    "\n",
    "        # list comp version: this doest work either\n",
    "#         b0 = [pm.Normal('b0_mfr_{}'.format(m),\n",
    "#                         mu=T.sum(b0_parent[mfr_parent_map[m]]),\n",
    "#                         sd=b0_mfr_sd) for m in range(38)]\n",
    "\n",
    "        b1 = pm.Normal('b1_fuel_type[T.petrol]', mu=0, sd=10)\n",
    "        b2a = pm.Normal('b2a_trans[T.manual]', mu=0, sd=10)\n",
    "        b2b = pm.Normal('b2b_trans[T.semiauto]', mu=0, sd=10)\n",
    "        b3 = pm.Normal('b3_is_tdi[T.True]', mu=0, sd=10)\n",
    "        b4 = pm.Normal('b4_engine_capacity', mu=0, sd=10)\n",
    "        b5 = pm.Normal('b5_metric_combined', mu=0, sd=10)\n",
    "        b6 = pm.Normal('b6_emissions_co_mgkm', mu=0, sd=10)\n",
    "                             \n",
    "        # define hierachical linear model\n",
    "        yest = ( b0[dfs['mfr_enc']] +\n",
    "                 b1 * mx_ex['fuel_type[T.petrol]'] + \n",
    "                 b2a * mx_ex['trans[T.manual]'] +\n",
    "                 b2b * mx_ex['trans[T.semiauto]'] +\n",
    "                 b3 * mx_ex['is_tdi[T.True]'] +\n",
    "                 b4 * mx_ex['engine_capacity'] +\n",
    "                 b5 * mx_ex['metric_combined'] +\n",
    "                 b6 * mx_ex['emissions_co_mgkm'])\n",
    "\n",
    "        ## StudentT likelihood with fixed degrees of freedom nu\n",
    "        epsilon = pm.HalfCauchy('epsilon', beta=10)\n",
    "        likelihood = pm.StudentT('likelihood', nu=1, mu=yest\n",
    "                                 ,sd=epsilon, observed=dfs[ft_endog])\n",
    "\n",
    "        time_mdl_hier_pymc['t0_build'] = time() - t0\n",
    "        \n",
    "        #start_MAP = pm.find_MAP(fmin=optimize.fmin_powell)  ## takes a LONG time\n",
    "        \n",
    "        time_mdl_hier_pymc['t1_findmap'] = time() - time_mdl_hier_pymc['t0_build']\n",
    "        \n",
    "        ## sample\n",
    "        trc_hier_pymc = pm.sample(4000, njobs=2, step=pm.NUTS()\n",
    "                               #,start=start_MAP\n",
    "                               ,trace=pm.backends.Text('traces/trc_hier_pymc'))\n",
    "\n",
    "        time_mdl_hier_pymc['t2_sample'] = time() - time_mdl_hier_pymc['t1_findmap']\n",
    "        \n",
    "    ## dump to disk\n",
    "    write_pickle(obj=mdl_hier_pymc, relnm='models/mdl_hier_pymc.pkl')\n",
    "\n",
    "else:\n",
    "    with pm.Model():   ## read from disk\n",
    "\n",
    "        mdl_hier_pymc = read_pickle(relnm='models/mdl_hier_pymc.pkl')\n",
    "\n",
    "    trc_hier_pymc = pm.backends.text.load('traces/trc_hier_pymc',\n",
    "                                          model=mdl_hier_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save WAIC and view traceplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dfwaic_pymc['hier'] = [pm.stats.waic(model=mdl_hier_pymc,\n",
    "#                                          trace=trc_hier_pymc[-500:])]\n",
    "rvs_hier_pymc = [rv.name for rv in strip_derived_rvs(mdl_hier_pymc.unobserved_RVs)]\n",
    "plot_traces_pymc(trc_hier_pymc[-500:], varnames=rvs_hier_pymc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyStan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_hier_stan_spec = \"\"\"\n",
    "\n",
    "    /* Spec for hierarchical regression linear model */\n",
    "\n",
    "    data {                                 \n",
    "        int<lower=0> N;                         // count of observations\n",
    "        int<lower=0> K;                         // count of exog features\n",
    "        matrix[N, K] X;                         // exog features\n",
    "        vector[N] y;                            // endog feature\n",
    "\n",
    "        int<lower=0> n_parent;                      // count of parent index levels\n",
    "        int<lower=1, upper=n_parent> parent_enc[N]; // parent index encoding  \n",
    "        \n",
    "        int<lower=0> n_mfr;                     // count of mfr index levels\n",
    "        int<lower=1, upper=n_mfr> mfr_enc[N];   // mfr index encoding          \n",
    "        \n",
    "        int<lower=1, upper=n_mfr> mfr_parent_map[n_mfr];\n",
    "    }\n",
    "    parameters {\n",
    "        vector[K] beta;                         // exog coeffs\n",
    "        real<lower=0> sigma;                    // linear model error       \n",
    "\n",
    "        real parent_mu;                         // parent mu hyperprior\n",
    "        real<lower=0> parent_sd;                // parent sd hyperprior\n",
    "\n",
    "        vector [n_parent] b0_parent;            // mfr mu hyperprior (parent prior)\n",
    "        real<lower=0> mfr_sd;                   // mfr sd hyperprior\n",
    "\n",
    "        vector[n_mfr] b0_mfr;                   // mfr prior\n",
    "    }\n",
    "    transformed parameters {}\n",
    "    model {  \n",
    "        \n",
    "        parent_mu ~ normal(0, 10);              // weakly informative\n",
    "        parent_sd ~ cauchy(0, 10);              // weakly informative\n",
    "        mfr_sd ~ cauchy(0, 10);                 // weakly informative\n",
    "        \n",
    "                  \n",
    "        for (parent in 1:n_parent) {            // parent priors (20x)\n",
    "              b0_parent[parent] ~ normal(parent_mu, parent_sd);\n",
    "        }\n",
    "\n",
    "        for (mfr in 1:n_mfr) {                  // mfr priors (38x)\n",
    "            b0_mfr[mfr] ~ normal(b0_parent[mfr_parent_map[mfr]], mfr_sd);\n",
    "        }      \n",
    "\n",
    "        sigma ~ cauchy(0, 10);                  // weakly informative noise\n",
    "        y ~ student_t(1, b0_mfr[mfr_enc] + X * beta, sigma);    // likelihood\n",
    "    }\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl_hier_stan_dict = {}\n",
    "mdl_hier_stan_dict['N'] = mx_ex.iloc[:,1:].shape[0]\n",
    "mdl_hier_stan_dict['K'] = mx_ex.iloc[:,1:].shape[1]\n",
    "mdl_hier_stan_dict['X'] = mx_ex.iloc[:,1:].values\n",
    "mdl_hier_stan_dict['y'] = mx_en[ft_endog].values\n",
    "\n",
    "mdl_hier_stan_dict['parent_enc'] = dfs['parent_enc'].values + 1\n",
    "mdl_hier_stan_dict['n_parent'] = n_parent\n",
    "mdl_hier_stan_dict['mfr_enc'] = dfs['mfr_enc'].values + 1\n",
    "mdl_hier_stan_dict['n_mfr'] = n_mfr\n",
    "\n",
    "mdl_hier_stan_dict['mfr_parent_map'] = mfr_parent_map + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if sample_switches['hier']['stan']:\n",
    "\n",
    "    mdl_hier_stan_fit = pystan.stan(\n",
    "            model_code  = mdl_hier_stan_spec,\n",
    "            data        = mdl_hier_stan_dict,\n",
    "            sample_file = ensure_dir('traces/trc_hier_stan/chain'),\n",
    "            model_name  = 'mdl_hier_stan',\n",
    "            iter        = 1000,\n",
    "            warmup      = 750, \n",
    "            chains      = 4, \n",
    "            n_jobs      = 2, \n",
    "            verbose     = False)\n",
    "    \n",
    "    # dump model object and model fit object to disk\n",
    "    write_pickle(obj=mdl_hier_stan_fit.get_stanmodel(),\n",
    "                 relnm='models/mdl_hier_stan_model.pkl')\n",
    "\n",
    "    write_pickle(mdl_hier_stan_fit, 'models/mdl_hier_stan_fit.pkl')\n",
    "    \n",
    "else:      \n",
    "    ## load the model first, and then the fit:\n",
    "    mdl_hier_stan_model = read_pickle(relnm='models/mdl_hier_stan_model.pkl')\n",
    "\n",
    "    mdl_hier_stan_fit = read_pickle(relnm='models/mdl_hier_stan_fit.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mdl_hier_stan_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = mdl_hier_stan_fit.plot()\n",
    "f.set_size_inches(12, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot all this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate manufacturer using the multilevel hierarchical model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare to unpooled and partpooled models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsm_hier_mfr = create_smry(trc_hier_pymc[-333:], dfs, 'mfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsm_mfr_vs = pd.concat((dfsm_hier_mfr\n",
    "                         ,dfsm_ptpl_mfr.reindex(dfsm_hier_mfr.index)\n",
    "                         ,dfsm_unpl_mfr.reindex(dfsm_hier_mfr.index)), axis=0)\n",
    "dfsm_mfr_vs['mdl'] = np.concatenate(\n",
    "        (np.repeat(['hier'],38), np.repeat(['partpooled'],38), np.repeat(['unpooled'],38)))\n",
    "dfsm_mfr_vs.iloc[38:]['ypos'] = dfsm_mfr_vs.iloc[:38]['ypos']\n",
    "dfsm_mfr_vs.iloc[2*38:]['ypos'] = dfsm_mfr_vs.iloc[:38]['ypos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "custom_forestplot(dfsm_mfr_vs, sg_mfrfreq, aspect=0.5, facetby='mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate parent using the multilevel hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfsm_hier_parent = create_smry(trc_hier_pymc[-500:], dfs, 'parent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sg_parentfreq = dfs.groupby('parent').size().reindex(dfsm_hier_parent.index)\n",
    "custom_forestplot(dfsm_hier_parent, sg_parentfreq, ylabel='parent', size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe:**\n",
    "\n",
    "+ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate parent-manufacturer using the multilevel hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup grouping structure\n",
    "dfg = dfs.groupby(['parent','mfr']).size().reset_index()\n",
    "dfg.rename(columns={0:'n'}, inplace=True)\n",
    "dfg['key'] = dfg[['parent','mfr']].apply(lambda r: '{} - {}'.format(r[0][:5], r[1]), axis=1)\n",
    "\n",
    "# join mfr\n",
    "dfg = pd.merge(dfg, dfsm_hier_mfr[['mean','hpd_2.5','hpd_97.5']]\n",
    "                       ,how='left', left_on='mfr', right_index=True)\n",
    "dfg.rename(columns={k:'{}_mfr'.format(k) for k in ['mean','hpd_2.5','hpd_97.5']}\n",
    "           ,inplace=True)\n",
    "\n",
    "# join parent\n",
    "dfg = pd.merge(dfg, dfsm_hier_parent[['mean','hpd_2.5','hpd_97.5']]\n",
    "                       ,how='left', left_on='parent', right_index=True)\n",
    "dfg.rename(columns={k:'{}_parent'.format(k) for k in ['mean','hpd_2.5','hpd_97.5']}\n",
    "           ,inplace=True)\n",
    "\n",
    "dfg.sort_values(['mean_parent','mean_mfr'], ascending=False, inplace=True)\n",
    "\n",
    "dfg['parent_order_id'] = pd.factorize(dfg['parent'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interactive(custom_2d_forestplot, dfg=fixed(dfg), show=['All','Zoom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**&copy; Applied AI Ltd 2016**  \n",
    "<a href='http://www.applied.ai'>applied.ai</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
